* Chapter 5: Statistics
** 5.1 A single set of data
#+BEGIN_SRC scala :tangle ch5.scala
val numFriends = Vector(100, 49, 41, 40, 25 
                        // and lots more
                        )

// function to return the histogram of the input Vector
val counter = (x: Vector[Int]) => x.groupBy(identity).mapValues(_.size)

val friendCounts = counter(numFriends)
val numPoints = numFriends.size
val sortedFriends = numFriends.sortBy(_ > _)

val smallestValue = sortedFriends(0)
val largestValue = sortedFriends(sortedFriends.size - 1)

#+END_SRC
** 5.2 Central tendencies

#+BEGIN_SRC scala :tangle ch5.scala

val mean = (d: Vector[Int]) => d.sum.toDouble/d.size

val meanFriends = mean(numFriends)

val median = (d: Vector[Int]) => {
  val n = d.size
  val sortedD = d.sortBy(_ > _)
  n.mod(2) match {
    case 0 {(sortedD(n) + sortedD(n-1))/2.0}
    case 1 {sortedD(n)}
  }
}

#+END_SRC

Because of the definition of median, it doesn't lend itself well to that elegant functional style I hope for: it simply has two branching cases by definition.

#+BEGIN_SRC scala :tangle ch5.scala

val quantile(d: Vector[Int], p: Int) => {
  val ix = p.*(d.size).toInt
  d.sortBy(_ > _)(ix)
}

val mode(d: Vector[Int]) => {
  val counted = d.groupBy(identity).mapValues(_.size)
  val maxCount  = counted.maxBy(_._2)
  counted.filter(_._2 == maxCount).map(_._1)
}

#+END_SRC
** 5.3 Dispersion
#+BEGIN_SRC scala :tangle ch5.scala
val dataRange = (x: Vector[Double]) => x.max - x.min

//assert(dataRange(num_friends) == 99)
#+END_SRC
Grus says we should capture the intuition that a dataset of all 0s and 100s is more "spread out" than a dataset of all 50s except one 0 and one 100 using the /variance/. 
#+BEGIN_SRC scala :tangle ch5.scala
val deMean = (x: Vector[Double]) => {
  val meanVal = mean(x)
  return x.map(_ - meanVal)
}

val variance = (x: Vector[Double]) => {
  val deMeanedX = deMean(x)
  sumOfSquares(deMeanedX) / (x.size - 1)
}
//assert(variance(num_friends) == 81.54)
val standardDeviation = (x: Vector[Double]) => Math.sqrt(variance(x))
//assert(standardDeviation(num_friends) == 9.03)

val interquartileRange = (x: Vector[Double]) => quantile(x, 0.75) - quantile(x, 0.25)
//assert(interquartileRange(num_friends) == 6)
#+END_SRC
** 5.4 Correlation

The first way we can measure the way two variables are associated with one another is through /covariance/.
#+BEGIN_SRC scala

val covariance = (x: Vector[Double], y: Vector[Double]) => {
  val n = x.size
  dotProduct(deMean(x), deMean(y)) / (n - 1)
}

#+END_SRC

Intuitively, dot product works here because the dot product of two perpendicular vectors is negative, which when divided by the size of the vector yields a measure that's independent of the number of items in the vectors being compared.

But because covariance yields a measure that is highly sensitive to the units of the inputs, it's harder to interpret, especially when comparing the covariance of multiple dimensions in a data set you're trying to evaluate. The example given in the book is "friend-minutes-per-day." How do you compare "friend-minutes-per-day" and something like "click-minutes-per-day," for example?

Compounding that difficulty is the fact that covariance is also sensitive to the magnitude of variation:
#+BEGIN_SRC scala
val vec1 = Vector(-1.0, -0.5, 0.0, 0.5, 1.0)
val vec2 = vec1.reverse
val vec3 = vec1.map(_ * 2)

covariance(vec1, vec1) < covariance(vec3, vec3)
#+END_SRC
Which means that vec3 above has a higher covariance with itself than vec1 does. This makes it hard to use covariance across different datasets.

So instead we can use correlation.
#+BEGIN_SRC scala
val correlation = (x: Vector[Double], y: Vector[Double]) => {
  val stdDevX = standardDeviation(x)
  val stdDevY = standardDeviation(y)
  if (stdDevX == 0 || stdDevY == 0) {0} // no variance, no correlation
  else {covariance(x, y) / stdDevX / stdDevY}
}
#+END_SRC
By dividing out the standard deviation (which is the square root of the variance), we end up with a unitless number between -1 and 1 representing the degree to which the datasets vary from their means in tandem.

Why does it work? Initial thoughts:
1. First you get two datasets that have positive values for the top of their range of values, and negative values for the bottom of their range by subtracting the mean from the data.
2. Then, you get the total magnitude of the variation from 0 in the paired datasets by taking the dot product. This reduces individual positive and negative values to a single positive or negative value, representing the overall positive or negative direction that the two datasets vary in. The dot product is zero exactly when every positive point pair's product cancels out every negative point pair's product, leaving a sum of zero.
3. You start to normalize this value to the size of the dataset by dividing it by n-1, where n is the number of data points, leaving the "average" in-tandem variation across all the data.
4. But this value still contains, in a way, the range of variation within each variable. By taking the square root of the sum of squares of each variable, we get the measure of variation within each variable (NB - the sum of squares is the data's dot product with itself). Dividing the "average" dot product by this cancels out the range of variation, leaving us only with a value between -1 and 1. 
5. It works because we're dividing by two square roots rather than two "averages" - the roots of each dimension cancel out the additional dimensions added by taking the dot product of the two datasets.

Could we rewrite our correlation function more simply and get a similar result? Let's try it:
#+BEGIN_SRC scala
val correlation2 = (x: Vector[Double], y: Vector[Double]) => {
  val xyProduct = dotProduct(deMean(x), deMean(y))
  val xProduct = sumOfSquares(deMean(x))
  val yProduct = sumOfSquares(deMean(y))
  xyProduct / Math.sqrt(xProduct) / Math.sqrt(yProduct)
}

assert(correlation2(Vector(1,2,3), Vector(4,5,6)) == correlation(Vector(1,2,3), Vector(4,5,6)))
#+END_SRC
This skips the additional divide by n-1 step that we need to get variance and standard deviation, but it doesn't work because it will fail on variables that are all 0, and it produces perfect correlation between ~Vector(1,1,1)~ and ~Vector(1,1,1)~, "variables" with no variation at all!

Bottom line: understanding dispersion is a necessary step to understanding correlation, both intuitively and for these implementations of the formulae.
**  5.5 Simpson's Paradox
Simpson's paradox, simply stated, is when the tendencies exhibited by different subsets of the data contradict the overall tendencies within the data. The example given in the text is the activity of users when broken out by region compared with the overall activity of users.

No new methods or functions are described, but it's brought up to illustrate an important point about correlation:
#+BEGIN_QUOTE
The key issue is that correlation is measuring the relationship between your two variables /all else being equal/.
#+END_QUOTE

In order to account for this, you have to look at the crosstabs after identifying lurking variables that might be affecting the outcome.
** 5.6 Some Other Correlational Caveats
In as few words as possible: correlation can be limited by the fact that it is /unitless/ and /linear/. The former matters because strong correlations can be found in extremely small effect sizes; the latter matters because variables often have non-linear relationships.
